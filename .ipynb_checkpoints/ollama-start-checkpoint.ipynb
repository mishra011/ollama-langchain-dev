{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b24fde-8c8f-43a3-97f1-a0d3846adc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepakmishra/work/env/dev/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you laugh! Do you want to hear another one?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3\"\n",
    ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f60222-6ccf-4035-ab54-103e7b3cbea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " scientists\n",
      " trust\n",
      " atoms\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " make\n",
      " up\n",
      " everything\n",
      "!\n",
      "\n",
      "\n",
      "Hope\n",
      " that\n",
      " made\n",
      " you\n",
      " smile\n",
      "!\n",
      " Do\n",
      " you\n",
      " want\n",
      " to\n",
      " hear\n",
      " another\n",
      " one\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me a joke\"\n",
    "\n",
    "for chunks in llm.stream(query):\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5520b29b-c8c5-48d3-8d28-c60311c289aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "type query here.....>>>>> hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### RESPONSE ###  Hi! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n",
      "TIME TAKEN :::  1.49263596534729\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "type query here.....>>>>> PM of India\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### RESPONSE ###  The Prime Minister (PM) of India is the head of government and the chief executive of the country. The current Prime Minister of India is Narendra Damodar Dasani, also known as Narendra Modi.\n",
      "\n",
      "Here are some key facts about the Prime Minister of India:\n",
      "\n",
      "1. **Appointment**: The Prime Minister is appointed by the President of India, who is the ceremonial head of state.\n",
      "2. **Term**: The Prime Minister serves a maximum term of five years, unless they resign or are removed from office.\n",
      "3. **Powers**: The Prime Minister has significant powers, including:\n",
      "\t* Leading the Council of Ministers (the cabinet)\n",
      "\t* Approving laws passed by Parliament\n",
      "\t* Appointing judges to the Supreme Court and High Courts\n",
      "\t* Signing international treaties on behalf of India\n",
      "4. **Responsibilities**: The Prime Minister is responsible for:\n",
      "\t* Formulating government policies and programs\n",
      "\t* Managing the country's economy\n",
      "\t* Maintaining national security\n",
      "\t* Representing India internationally\n",
      "\n",
      "Some notable Prime Ministers of India include:\n",
      "\n",
      "1. Jawaharlal Nehru (1947-1964): Known as the \"Architect of Modern India,\" he played a key role in India's independence movement and was the first Prime Minister.\n",
      "2. Indira Gandhi (1966-1977, 1980-1984): She was the world's first female Prime Minister and led India during times of national crisis, including the Bangladesh Liberation War.\n",
      "3. Rajiv Gandhi (1984-1989): He is known for his economic reforms and his role in promoting Indian IT industry.\n",
      "\n",
      "These are just a few examples of the many notable Prime Ministers of India.\n",
      "TIME TAKEN :::  18.55202889442444\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "type query here.....>>>>> python code of fibbonaci\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### RESPONSE ###  Here is a simple implementation of the Fibonacci sequence in Python:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        a, b = 0, 1\n",
      "        for i in range(2, n+1):\n",
      "            a, b = b, a+b\n",
      "        return b\n",
      "\n",
      "# Test the function\n",
      "for i in range(10):\n",
      "    print(fibonacci(i))\n",
      "```\n",
      "This code uses a recursive approach to calculate the Fibonacci numbers. The `fibonacci` function takes an integer `n` as input and returns the `n`-th Fibonacci number.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. If `n` is 0 or 1, return `n` immediately (base case).\n",
      "2. Initialize two variables `a` and `b` to 0 and 1, respectively.\n",
      "3. Loop from 2 to `n+1`, updating the values of `a` and `b` in each iteration:\n",
      "\t* In each iteration, set `a` to the previous value of `b`, and set `b` to the sum of the previous values of `a` and `b`.\n",
      "4. Return the final value of `b`, which is the `n`-th Fibonacci number.\n",
      "\n",
      "You can test this function by calling it with different values of `n`, like this:\n",
      "```\n",
      "for i in range(10):\n",
      "    print(fibonacci(i))\n",
      "```\n",
      "This will print the first 10 Fibonacci numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.\n",
      "\n",
      "Note that this implementation has a time complexity of O(n), which means it becomes slower as `n` increases. For larger values of `n`, you may want to use a more efficient algorithm or a precomputed table of Fibonacci numbers.\n",
      "TIME TAKEN :::  21.16471290588379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#llm = ChatOllama(model=\"llama3\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype query here.....>>>>>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "File \u001b[0;32m~/work/env/dev/lib/python3.9/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/env/dev/lib/python3.9/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import time\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "#llm = ChatOllama(model=\"llama3\")\n",
    "while True:\n",
    "    query = input(\"type query here.....>>>>>\")\n",
    "    start = time.time()\n",
    "    response = llm.invoke(query)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\" ### RESPONSE ### \", response)\n",
    "    print(\"TIME TAKEN ::: \", end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82b14bb-bfa2-4316-be63-d7ef0a0a8c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the astronaut break up with his girlfriend?\n",
      "\n",
      "Because he needed space!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/concepts/#langchain-expression-language-lcel\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756ae2f5-1906-4f4c-a395-621e7ccc479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why\n",
      " did\n",
      " the\n",
      " astronaut\n",
      " break\n",
      " up\n",
      " with\n",
      " his\n",
      " girlfriend\n",
      " before\n",
      " going\n",
      " to\n",
      " Mars\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " he\n",
      " needed\n",
      " space\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic = {\"topic\": \"Space travel\"}\n",
    "\n",
    "async for chunks in chain.astream(topic):\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89b1e7bf-007a-43d6-9868-41e678b9ec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided JSON schema:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": {\n",
      "    \"$type\": \"string\"\n",
      "  },\n",
      "  \"age\": {\n",
      "    \"$type\": \"integer\"\n",
      "  },\n",
      "  \"hobbies\": {\n",
      "    \"$type\": [\"array\", \"object\"],\n",
      "    \"$child\": {\n",
      "      \"$type\": \"string\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Here's the information about John:\n",
      "\n",
      "* Name: John\n",
      "* Age: 35 (an integer)\n",
      "* Hobbies: pizza (a string)\n",
      "\n",
      "So, John is a 35-year-old person who loves pizza!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"fav_food\": {\n",
    "            \"title\": \"Fav Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"],\n",
    "}\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Please tell me about a person using the following JSON schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{dumps}\"),\n",
    "    HumanMessage(\n",
    "        content=\"Now, considering the schema, tell me about a person named John who is 35 years old and loves pizza.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "dumps = json.dumps(json_schema, indent=2)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"dumps\": dumps}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e9857-a4ae-4bed-99d4-6983346c3a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
